package edu.carleton.comp4601.crawlerstuff;

import java.io.IOException;
import java.net.URL;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.Set;
import java.util.regex.Pattern;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.BodyContentHandler;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;
import org.xml.sax.SAXException;

import edu.carleton.comp4601.dao.DocumentCollection;
import edu.carleton.comp4601.graphstuff.MyGraph;
import edu.carleton.comp4601.graphstuff.MyVertex;
import edu.uci.ics.crawler4j.crawler.Page;
import edu.uci.ics.crawler4j.crawler.WebCrawler;
import edu.uci.ics.crawler4j.parser.HtmlParseData;
import edu.uci.ics.crawler4j.url.WebURL;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;


public class MyCrawler extends WebCrawler {

    private final static Pattern FILTERS = Pattern.compile(".*(\\.(css|js|gif|jpg"
                                                           + "|png|mp3|mp4|zip|gz))$");

    //Storage Variable(s)
    DocumentCollection documents = DocumentCollection.getInstance();
    
    //Graph variable(s)
    private MyGraph graph = MyGraph.getInstance();
    
    //Adaptive crawling variable(s)
    long startTime;
    long endTime;
    
    /**
     * This method receives two parameters. The first parameter is the page
     * in which we have discovered this new url and the second parameter is
     * the new url. You should implement this function to specify whether
     * the given url should be crawled or not (based on your crawling logic).
     * In this example, we are instructing the crawler to ignore urls that
     * have css, js, git, ... extensions and to only accept urls that start
     * with "http://www.ics.uci.edu/". In this case, we didn't need the
     * referringPage parameter to make the decision.
     */
     @Override
     public boolean shouldVisit(Page referringPage, WebURL url) {
    	//Start the adaptive stopwatch...
         startTime = System.nanoTime();
    	 
         String href = url.getURL().toLowerCase();
         
         //Filters out which pages should be visited
         return !FILTERS.matcher(href).matches()
                 && (href.startsWith("http://www.ics.uci.edu/") || 
                     href.startsWith("https://sikaman.dyndns.org/courses/4601/resources/") ||
                     href.startsWith("https://sikaman.dyndns.org/courses/4601/handouts/"));
                    
     }

     /**
      * This function is called when a page is fetched and ready
      * to be processed by your program.
      */
     @Override
     public void visit(Page page) {
    	//Store the URL
         String url    = page.getWebURL().getURL();
         String parent = page.getWebURL().getParentUrl();
    	 
        //Create the new Document (and some varaibles to help its creation)
    	 edu.carleton.comp4601.dao.Document newDocument = new edu.carleton.comp4601.dao.Document();
         String content = "";
    	 boolean htmlPage = false;
         
         System.out.println("URL: " + url);
         
         //Variables used to fill Document (later)
         String text                 = null;
         Set<WebURL> links           = null;
         HtmlParseData htmlParseData = null;
         String tikaParseddata       = null;
         String tikaMetadata         = null;
         
         //if HTML
         if (page.getParseData() instanceof HtmlParseData) {
        	 htmlParseData = (HtmlParseData) page.getParseData();
        	 links = htmlParseData.getOutgoingUrls();
        	 
        	 //If HTML use JSoup to parse....
        	 try {
				Document doc = Jsoup.connect(url).get();
				String title = doc.title();
				String body = doc.body().text();                //JSOUP requirment was very unspecific....hope this is enough
				Elements jlinks = doc.select("a[href]");
				
				List<String> linkList = new ArrayList<String>();
				for (Element link : jlinks) {
					String href = link.attr("href");
					linkList.add(href);
					newDocument.getLinks().add(href);
				}
				
				content =title + "\n" + body;                  //This is how we set Text in newDocument
			 
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
        	 
        	 htmlPage = true;
         }
         
        
         //MyVertex creation
         MyVertex cVertex = new MyVertex(url);
         MyVertex parentVertex = null;
         if (parent != null && (parent.compareTo(url) != 0)){ parentVertex = new MyVertex(parent);}
         
         //Add MyVertex to Graph
         try { graph.addVertex(cVertex, parentVertex);} 
         catch (IOException e) { e.printStackTrace(); }  
         
     
     	//Create docID
     	int docID = documents.produceID();
     
     	//Extract all metadata/otherdata using Tika on html and non html files
     	//--Tika basic setup
     	AutoDetectParser parser = new AutoDetectParser();
     	BodyContentHandler handler = new BodyContentHandler();
     	Metadata metadata = new Metadata();
     	
     	//--Get input method ready
		try {
			TikaInputStream stream;
			URL tikaURL = new URL(url);
			stream = TikaInputStream.get(tikaURL);
			
	    //--Get output ready
         	ParseContext context =new ParseContext();
        
        //--Parse
         	parser.parse(stream, handler, metadata, context);
         	
        tikaParseddata = handler.toString();
        tikaMetadata   = metadata.toString(); 
        if(htmlPage==false){content = tikaParseddata;}  //This is where we replace the HTML JSoup content parser with the Tika content parser
        
		} catch (IOException | SAXException | TikaException e) { e.printStackTrace();} 
     	
     	//(Assumed Step) Populate Document
		newDocument.setId(docID);
     	newDocument.setUrl(url);
     	newDocument.setName("webpage " + docID);
     	newDocument.setText(content);
     	SimpleDateFormat formatter = new SimpleDateFormat("dd/MM/yyyy HH:mm:ss");  
        Date date = new Date();   
     	newDocument.setDate(formatter.format(date));
     	String[] metadataNames = metadata.names();

     	//Stop adaptive stopwatch...
     	endTime = System.nanoTime();
     	
     	ArrayList<String> metaNameArray = new ArrayList<String>();
     	ArrayList<String> metaDataArray = new ArrayList<String>();;
        for(String name : metadataNames) {		        
           metaNameArray.add(name);
           metaDataArray.add(metadata.get(name));
        }
        newDocument.setMetaname(metaNameArray);
        newDocument.setMetadata(metaDataArray);
     	
     	
                 
		
		 
		//Create adaptive pause
        if(startTime==0){startTime=endTime;}
		long stopwatch = (endTime - startTime)/1000000; //converts to milliseconds
		//Upload Document to MongoDB
     	documents.add(newDocument);
		System.out.println("STOPWATCH TIME: " +  stopwatch/1000 + "    " + "s:"+startTime + " e:"+endTime);
		try { Thread.sleep(stopwatch*0); } 
		catch (InterruptedException e) { e.printStackTrace(); }
     }
     
     
             
    }
